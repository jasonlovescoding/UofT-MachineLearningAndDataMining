{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1847)\n",
    "\n",
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "\n",
    "    You shouldn't need to touch this.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        '''\n",
    "        Get random batch indices without replacement from the dataset.\n",
    "\n",
    "        If m is given the batch will be of size m. Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "\n",
    "        If m is given the batch will be of size m. Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GDOptimizer(object):\n",
    "    '''\n",
    "    A gradient descent optimizer with momentum\n",
    "    '''\n",
    "\n",
    "    def __init__(self, lr, beta=0.0):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.delta = 0\n",
    "        \n",
    "    def update_params(self, params, grad):\n",
    "        # Update parameters using GD with momentum and return\n",
    "        # the updated parameters\n",
    "        delta = -self.lr * grad + self.beta * self.delta\n",
    "        self.delta = delta\n",
    "        return params + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    '''\n",
    "    A Support Vector Machine\n",
    "    '''\n",
    "\n",
    "    def __init__(self, c, feature_count):\n",
    "        self.c = c\n",
    "        self.w = np.random.normal(0.0, 0.1, feature_count)\n",
    "        \n",
    "    def hinge_loss(self, X, y):\n",
    "        '''\n",
    "        Compute the hinge-loss for input data X (shape (n, m)) with target y (shape (n,)).\n",
    "\n",
    "        Returns a length-n vector containing the hinge-loss per data point.\n",
    "        '''\n",
    "        # Implement hinge loss\n",
    "        loss = 1 - y * (np.dot(X, self.w))\n",
    "        loss[loss <= 0] = 0\n",
    "        return loss\n",
    "\n",
    "    def grad(self, X, y):\n",
    "        '''\n",
    "        Compute the gradient of the SVM objective for input data X (shape (n, m))\n",
    "        with target y (shape (n,))\n",
    "\n",
    "        Returns the gradient with respect to the SVM parameters (shape (m,)).\n",
    "        '''\n",
    "        # Compute (sub-)gradient of SVM objective\n",
    "        loss = 1 - y * (np.dot(X, self.w))\n",
    "        idx = np.where(loss > 0)[0]\n",
    "        y_miss = y[idx]\n",
    "        X_miss = X[idx]\n",
    "        penalty = np.mean(y_miss.reshape(y_miss.shape[0], 1) * X_miss, axis=0)\n",
    "        w = np.insert(self.w[1:], 0, 0) # exclude bias term\n",
    "        return w - self.c * penalty\n",
    "\n",
    "    def classify(self, X):\n",
    "        '''\n",
    "        Classify new input data matrix (shape (n,m)).\n",
    "\n",
    "        Returns the predicted class labels (shape (n,))\n",
    "        '''\n",
    "        # Classify points as +1 or -1\n",
    "        pred = np.dot(X, self.w)\n",
    "        pred[pred > 0] = 1\n",
    "        pred[pred <= 0] = -1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Load MNIST data (4 and 9 only) and split into train and test\n",
    "    '''\n",
    "    mnist = fetch_mldata('MNIST original', data_home='./data')\n",
    "    label_4 = (mnist.target == 4)\n",
    "    label_9 = (mnist.target == 9)\n",
    "\n",
    "    data_4, targets_4 = mnist.data[label_4], np.ones(np.sum(label_4))\n",
    "    data_9, targets_9 = mnist.data[label_9], -np.ones(np.sum(label_9))\n",
    "\n",
    "    data = np.concatenate([data_4, data_9], 0)\n",
    "    data = data / 255.0\n",
    "    targets = np.concatenate([targets_4, targets_9], 0)\n",
    "\n",
    "    permuted = np.random.permutation(data.shape[0])\n",
    "    train_size = int(np.floor(data.shape[0] * 0.8))\n",
    "\n",
    "    train_data, train_targets = data[permuted[:train_size]], targets[permuted[:train_size]]\n",
    "    test_data, test_targets = data[permuted[train_size:]], targets[permuted[train_size:]]\n",
    "    print(\"Data Loaded\")\n",
    "    print(\"Train size: {}\".format(train_size))\n",
    "    print(\"Test size: {}\".format(data.shape[0] - train_size))\n",
    "    print(\"-------------------------------\")\n",
    "    # add bias term\n",
    "    train_data = np.insert(train_data, 0, 1, axis=1)\n",
    "    test_data = np.insert(test_data, 0, 1, axis=1)\n",
    "    return train_data, train_targets, test_data, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_test_function(optimizer, w_init=10.0, steps=200):\n",
    "    '''\n",
    "    Optimize the simple quadratic test function and return the parameter history.\n",
    "    '''\n",
    "    def func(x):\n",
    "        return 0.01 * x * x\n",
    "\n",
    "    def func_grad(x):\n",
    "        return 0.02 * x\n",
    "\n",
    "    w = w_init\n",
    "    w_history = [w_init]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # Optimize and update the history\n",
    "        w = optimizer.update_params(w, func_grad(w))\n",
    "        w_history.append(w)\n",
    "    return w_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_svm(train_data, train_targets, penalty, optimizer, batch_size, iters):\n",
    "    '''\n",
    "    Optimize the SVM with the given hyperparameters. Return the trained SVM.\n",
    "    '''\n",
    "    sampler = BatchSampler(train_data, train_targets, batch_size)\n",
    "    svm = SVM(penalty, train_data.shape[1])\n",
    "    for _ in range(iters):\n",
    "        X, y = sampler.get_batch()\n",
    "        grad = svm.grad(X, y)\n",
    "        svm.w = optimizer.update_params(svm.w, grad)\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.0, beta = 0.0, w0 = 10.0, steps = 200, final w: 0.17587946605721566\n",
      "lr = 1.0, beta = 0.9, w0 = 10.0, steps = 200, final w: -2.0665349231570765e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer = GDOptimizer(lr = 1.0, beta = 0.0)\n",
    "print('lr = 1.0, beta = 0.0, w0 = 10.0, steps = 200, final w: ' + str(optimize_test_function(optimizer)[-1]))\n",
    "optimizer = GDOptimizer(lr = 1.0, beta = 0.9)\n",
    "print('lr = 1.0, beta = 0.9, w0 = 10.0, steps = 200, final w: ' + str(optimize_test_function(optimizer)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Train size: 11025\n",
      "Test size: 2757\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.05, C = 1.0, m = 100, T = 500, beta = 0.0\n",
      "training accuracy: 0.9172789115646258, test accuracy: 0.9133115705476967\n",
      "training loss: 0.30070056132858464, \n",
      "test loss: 0.30457613737347705\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfbDv9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114bcacc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = GDOptimizer(lr = 0.05, beta = 0.0)\n",
    "svm = optimize_svm(X_train, y_train, 1.0, optimizer, 100, 500)\n",
    "y_pred = svm.classify(X_train)\n",
    "train_acc = (y_pred == y_train).mean()\n",
    "train_loss = svm.hinge_loss(X_train, y_train)\n",
    "y_pred = svm.classify(X_test)\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "test_loss = svm.hinge_loss(X_test, y_test)\n",
    "print('alpha = 0.05, C = 1.0, m = 100, T = 500, beta = 0.0')\n",
    "print('training accuracy: {}, test accuracy: {}'.format(train_acc, test_acc))\n",
    "print('training loss: {}, \\ntest loss: {}'.format(np.mean(train_loss), np.mean(test_loss)))\n",
    "plt.imshow(svm.w[1:].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.05, C = 1.0, m = 100, T = 500, beta = 0.1\n",
      "training accuracy: 0.927891156462585, test accuracy: 0.9220166848023214\n",
      "training loss: 0.29247342549848304, \n",
      "test loss: 0.29536689916718345\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f889550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = GDOptimizer(lr = 0.05, beta = 0.1)\n",
    "svm = optimize_svm(X_train, y_train, 1.0, optimizer, 100, 500)\n",
    "y_pred = svm.classify(X_train)\n",
    "train_acc = (y_pred == y_train).mean()\n",
    "train_loss = svm.hinge_loss(X_train, y_train)\n",
    "y_pred = svm.classify(X_test)\n",
    "test_acc = (y_pred == y_test).mean()\n",
    "test_loss = svm.hinge_loss(X_test, y_test)\n",
    "print('alpha = 0.05, C = 1.0, m = 100, T = 500, beta = 0.1')\n",
    "print('training accuracy: {}, test accuracy: {}'.format(train_acc, test_acc))\n",
    "print('training loss: {}, \\ntest loss: {}'.format(np.mean(train_loss), np.mean(test_loss)))\n",
    "plt.imshow(svm.w[1:].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
